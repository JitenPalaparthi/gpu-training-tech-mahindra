
# üöÄ GPU-to-GPU Communication over Network Using RDMA

This process is also referred to as:

> **GPUDirect RDMA** ‚Äî direct memory access between GPUs across nodes **without CPU involvement**.

---

## üß† 1. What Is GPUDirect RDMA?

It‚Äôs a NVIDIA feature that enables RDMA NICs (like Mellanox ConnectX) to **directly read/write GPU memory** via PCIe, bypassing:
- The CPU
- System RAM
- Kernel driver stack

‚û°Ô∏è This enables **GPU-to-GPU data transfers across the network** with **very low latency** and **high throughput**.

---

## üèóÔ∏è 2. Basic Components in the Path

| Component             | Role |
|----------------------|------|
| GPU A                | Source of data |
| RNIC A               | RDMA NIC attached to Node A |
| Network              | RoCE/Infiniband transport |
| RNIC B               | RDMA NIC on Node B |
| GPU B                | Destination GPU |
| PCIe + DMA Engines   | Used by NICs to talk to GPUs directly |

---

## üîÑ 3. Step-by-Step: GPU-to-GPU via RDMA

### üí° Assume: Node A (GPU A), Node B (GPU B)

| Step | What Happens |
|------|--------------|
| 1Ô∏è‚É£ | Application on Node A registers GPU memory (using `cudaHostRegister`) |
| 2Ô∏è‚É£ | RNIC A maps that memory using **Memory Translation Table (MTT)** |
| 3Ô∏è‚É£ | Application on Node A initiates RDMA **write** or **send** |
| 4Ô∏è‚É£ | RNIC A uses DMA over PCIe to fetch data directly from GPU A memory |
| 5Ô∏è‚É£ | RNIC A transmits data over **RDMA fabric** (e.g., Infiniband, RoCEv2) |
| 6Ô∏è‚É£ | RNIC B receives data and performs DMA to write directly into GPU B memory |
| 7Ô∏è‚É£ | Completion Queues updated ‚Üí signaling done to both applications |

‚úÖ At no point is CPU RAM or kernel buffer involved.

---

## ‚öôÔ∏è 4. Key Requirements for This to Work

| Requirement | Details |
|-------------|---------|
| **GPUDirect RDMA Support** | Must be enabled in both GPUs and drivers |
| **CUDA-aware MPI / NCCL** | Libraries must support GPU buffers |
| **Proper IOMMU + PCIe topology** | NIC and GPU should be on the same PCIe root complex (ideal) |
| **Pinned Memory** | GPU memory must be pinned (registered) to prevent page faults |
| **Compatible NIC (RNIC)** | Mellanox ConnectX-4 or later |

---

## ‚ö° 5. Performance Benefits

| Metric | Traditional (CPU copy) | GPUDirect RDMA |
|--------|------------------------|----------------|
| Latency | 30‚Äì50 ¬µs              | 3‚Äì5 ¬µs         |
| CPU Usage | High                 | Near 0%        |
| Throughput | Lower (due to copies) | 40‚Äì100+ Gbps  |

---

## üìä Example Use Case

### Deep Learning Distributed Training (e.g., using PyTorch or TensorFlow + NCCL):
- Node A computes gradients on GPU
- Using NCCL over RDMA, these are sent directly to GPU B on Node B
- GPU B aggregates and backpropagates, no CPU needed
